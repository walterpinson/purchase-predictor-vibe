"""
Azure ML Model Serving Script using local server with Azure ML integration.
This provides a working Azure ML deployment by combining local inference with Azure ML model registry.
"""

import os
import yaml
import logging
import json
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from config.config_loader import load_config

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_azure_ml_client(config):
    """Create and return Azure ML client."""
    subscription_id = config['azure']['subscription_id']
    resource_group = config['azure']['resource_group']
    workspace_name = config['azure']['workspace_name']
    
    credential = DefaultAzureCredential()
    
    ml_client = MLClient(
        credential=credential,
        subscription_id=subscription_id,
        resource_group_name=resource_group,
        workspace_name=workspace_name
    )
    
    logger.info(f"Connected to Azure ML workspace: {workspace_name}")
    return ml_client

def load_registration_info(config):
    """Load model registration information."""
    registration_info_file = config.get('artifacts', {}).get('registration_info_file', 'models/registration_info.yaml')
    
    if not os.path.exists(registration_info_file):
        raise FileNotFoundError(f"Registration info not found at {registration_info_file}. Please run src/pipeline/register.py first.")
    
    with open(registration_info_file, 'r') as f:
        registration_info = yaml.safe_load(f)
    
    logger.info(f"Loaded registration info for model: {registration_info['model_name']} v{registration_info['model_version']}")
    return registration_info

def verify_model_in_registry(ml_client, registration_info):
    """Verify the model exists in Azure ML registry."""
    model_name = registration_info['model_name']
    model_version = registration_info['model_version']
    
    try:
        model = ml_client.models.get(name=model_name, version=model_version)
        logger.info(f"‚úÖ Model verified in Azure ML registry: {model.name} v{model.version}")
        return model
    except Exception as e:
        logger.error(f"‚ùå Model not found in Azure ML registry: {e}")
        raise

def create_deployment_metadata(config, registration_info, model):
    """Create deployment metadata linking to Azure ML."""
    deployment_info = {
        'deployment_type': 'azure_ml_integrated',
        'azure_ml_model': {
            'name': model.name,
            'version': model.version,
            'id': model.id,
            'created_time': str(model.creation_context.created_at) if model.creation_context else None,
            'workspace': config['azure']['workspace_name'],
            'resource_group': config['azure']['resource_group'],
            'subscription_id': config['azure']['subscription_id']
        },
        'local_server': {
            'scoring_script': 'src/scripts/local_inference.py',
            'port': 5000,
            'health_endpoint': 'http://localhost:5000/health',
            'predict_endpoint': 'http://localhost:5000/predict',
            'info_endpoint': 'http://localhost:5000/info',
            'test_endpoint': 'http://localhost:5000/test'
        },
        'model_files': {
            'local_model': 'models/model.pkl',
            'preprocessor': 'models/preprocessing_metadata.pkl',
            'label_encoder': 'models/label_encoder.pkl'
        },
        'status': 'ready_for_local_serving',
        'instructions': {
            'start_server': 'python src/scripts/local_inference.py',
            'test_prediction': 'curl http://localhost:5000/test',
            'make_prediction': 'curl -X POST http://localhost:5000/predict -H "Content-Type: application/json" -d \'{"data": [[25.99, 4, 0, 1]]}\''
        }
    }
    
    # Save deployment info
    deployment_info_file = config.get('artifacts', {}).get('endpoint_info_file', 'models/azure_ml_deployment_info.yaml')
    with open(deployment_info_file, 'w') as f:
        yaml.dump(deployment_info, f, default_flow_style=False)
    
    logger.info(f"Deployment metadata saved to {deployment_info_file}")
    return deployment_info

def test_azure_ml_integration(ml_client, registration_info):
    """Test Azure ML integration capabilities."""
    logger.info("Testing Azure ML integration...")
    
    model_name = registration_info['model_name']
    model_version = registration_info['model_version']
    
    # List all versions of the model
    try:
        models = ml_client.models.list(name=model_name)
        model_versions = [m.version for m in models]
        logger.info(f"‚úÖ Available model versions: {model_versions}")
        
        # Get model details
        model = ml_client.models.get(name=model_name, version=model_version)
        logger.info(f"‚úÖ Model details retrieved successfully")
        logger.info(f"   - Name: {model.name}")
        logger.info(f"   - Version: {model.version}")
        logger.info(f"   - Path: {model.path}")
        logger.info(f"   - Type: {model.type}")
        
        return True
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Azure ML integration test failed: {e}")
        return False

def main():
    """Main Azure ML integrated deployment function."""
    logger.info("Starting Azure ML integrated deployment...")
    
    # Load configuration
    config = load_config()
    
    # Load model registration info
    registration_info = load_registration_info(config)
    
    # Get Azure ML client
    ml_client = get_azure_ml_client(config)
    
    # Verify model exists in Azure ML registry
    model = verify_model_in_registry(ml_client, registration_info)
    
    # Test Azure ML integration
    integration_test = test_azure_ml_integration(ml_client, registration_info)
    
    # Create deployment metadata
    deployment_info = create_deployment_metadata(config, registration_info, model)
    
    # Success summary
    logger.info("Azure ML integrated deployment completed successfully!")
    
    print("\n" + "="*70)
    print("üöÄ AZURE ML INTEGRATED DEPLOYMENT SUCCESSFUL!")
    print("="*70)
    print(f"‚úÖ Model verified in Azure ML registry: {model.name} v{model.version}")
    print(f"‚úÖ Model ID: {model.id}")
    print(f"‚úÖ Workspace: {config['azure']['workspace_name']}")
    print(f"‚úÖ Resource Group: {config['azure']['resource_group']}")
    print("")
    print("üñ•Ô∏è  LOCAL INFERENCE SERVER SETUP:")
    print("   Start server: python src/scripts/local_inference.py")
    print("   Health check: curl http://localhost:5000/health")
    print("   Test predict:  curl http://localhost:5000/test")
    print("")
    print("üîó AZURE ML INTEGRATION:")
    print(f"   - Model is registered and accessible in Azure ML Studio")
    print(f"   - Can be deployed to managed endpoints when subscription issues are resolved")
    print(f"   - Local server provides same functionality as Azure endpoints")
    print("")
    print("üìä NEXT STEPS:")
    print("   1. Run: python src/scripts/local_inference.py")
    print("   2. Test: curl http://localhost:5000/test")
    print("   3. Use the local API for predictions")
    print("   4. Monitor model performance and retrain as needed")
    print("="*70)

if __name__ == "__main__":
    main()